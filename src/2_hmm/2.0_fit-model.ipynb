{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MYWEVoFZa69B",
    "outputId": "cf161140-e413-4d33-b0ae-50513e40555a"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  data = \"/content/drive/MyDrive/ComputationalGenomics/\"\n",
    "except:\n",
    "  data = \"/Users/fionshiau/Library/CloudStorage/GoogleDrive-fioncshiau@gmail.com/My Drive/ComputationalGenomics\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sgLxuWMJBP3F"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pomegranate;\n",
    "!pip install watermark;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "id": "Fi9ZJxAYCK10",
    "outputId": "3e7631c3-0bb0-4a47-d98e-fbf8f415d64a"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[39m.\u001b[39mrun_line_magic(\u001b[39m'\u001b[39m\u001b[39mmatplotlib\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39minline\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m; sns\u001b[39m.\u001b[39mset_style(\u001b[39m'\u001b[39m\u001b[39mwhitegrid\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set_style('whitegrid')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from hmmlearn.hmm import MultinomialHMM\n",
    "\n",
    "from pomegranate import *\n",
    "\n",
    "np.random.seed(0)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark -m -n -p numpy,scipy,pomegranate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = \"/Users/fionshiau/comp_geno\"\n",
    "obs = pd.read_csv(os.path.join(local_path,'merged','merged_1.csv'),index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9CYzVGa_u6hq"
   },
   "outputs": [],
   "source": [
    "chr1 = obs.loc[obs.chr=='chr1',:]\n",
    "chr1 = chr1.iloc[:,3:]\n",
    "chr1 = chr1.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1246254, 9)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4869, 14011,  2381,  4831,  6186,  2737, 95550, 27540,  1032])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr1.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function from_samples:\n",
      "\n",
      "from_samples(...) method of builtins.type instance\n",
      "    Learn the transitions and emissions of a model directly from data.\n",
      "    \n",
      "    This method will learn both the transition matrix, emission distributions,\n",
      "    and start probabilities for each state. This will only return a dense\n",
      "    graph without any silent states or explicit transitions to an end state.\n",
      "    Currently all components must be defined as the same distribution, but\n",
      "    soon this restriction will be removed.\n",
      "    \n",
      "    If learning a multinomial HMM over discrete characters, the initial\n",
      "    emisison probabilities are initialized randomly. If learning a\n",
      "    continuous valued HMM, such as a Gaussian HMM, then kmeans clustering\n",
      "    is used first to identify initial clusters.\n",
      "    \n",
      "    Regardless of the type of model, the transition matrix and start\n",
      "    probabilities are initialized uniformly. Then the specified learning\n",
      "    algorithm (Baum-Welch recommended) is used to refine the parameters\n",
      "    of the model.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    distribution : callable\n",
      "            The emission distribution of the components of the model.\n",
      "    \n",
      "    n_components : int\n",
      "            The number of states (or components) to initialize.\n",
      "    \n",
      "    X : array-like or generator\n",
      "            An array of some sort (list, numpy.ndarray, tuple..) of sequences,\n",
      "            where each sequence is a numpy array, which is 1 dimensional if\n",
      "            the HMM is a one dimensional array, or multidimensional if the HMM\n",
      "            supports multiple dimensions. Alternatively, a data generator\n",
      "            object that yields sequences.\n",
      "    \n",
      "    weights : array-like or None, optional\n",
      "            An array of weights, one for each sequence to train on. If None,\n",
      "            all sequences are equally weighted. Default is None.\n",
      "    \n",
      "    labels : array-like or None, optional\n",
      "            An array of state labels for each sequence. This is only used in\n",
      "            'labeled' training. If used this must be comprised of n lists where\n",
      "            n is the number of sequences to train on, and each of those lists\n",
      "            must have one label per observation. A None in this list corresponds\n",
      "            to no labels for the entire sequence and triggers semi-supervised\n",
      "            learning, where the labeled sequences are summarized using labeled\n",
      "            fitting and the unlabeled are summarized using the specified algorithm.\n",
      "            Default is None.\n",
      "    \n",
      "    algorithm : 'baum-welch', 'viterbi', 'labeled'\n",
      "            The training algorithm to use. Baum-Welch uses the forward-backward\n",
      "            algorithm to train using a version of structured EM. Viterbi\n",
      "            iteratively runs the sequences through the Viterbi algorithm and\n",
      "            then uses hard assignments of observations to states using that.\n",
      "            Default is 'baum-welch'. Labeled training requires that labels\n",
      "            are provided for each observation in each sequence.\n",
      "    \n",
      "    inertia : double or None, optional, range [0, 1]\n",
      "            If double, will set both edge_inertia and distribution_inertia to\n",
      "            be that value. If None, will not override those values. Default is\n",
      "            None.\n",
      "    \n",
      "    edge_inertia : bool, optional, range [0, 1]\n",
      "            Whether to use inertia when updating the transition probability\n",
      "            parameters. Default is 0.0.\n",
      "    \n",
      "    distribution_inertia : double, optional, range [0, 1]\n",
      "            Whether to use inertia when updating the distribution parameters.\n",
      "            Default is 0.0.\n",
      "    \n",
      "    pseudocount : double, optional\n",
      "            A pseudocount to add to both transitions and emissions. If supplied,\n",
      "            it will override both transition_pseudocount and emission_pseudocount\n",
      "            in the same way that specifying `inertia` will override both\n",
      "            `edge_inertia` and `distribution_inertia`. Default is None.\n",
      "    \n",
      "    transition_pseudocount : double, optional\n",
      "            A pseudocount to add to all transitions to add a prior to the\n",
      "            MLE estimate of the transition probability. Default is 0.\n",
      "    \n",
      "    emission_pseudocount : double, optional\n",
      "            A pseudocount to add to the emission of each distribution. This\n",
      "            effectively smoothes the states to prevent 0. probability symbols\n",
      "            if they don't happen to occur in the data. Only effects hidden\n",
      "            Markov models defined over discrete distributions. Default is 0.\n",
      "    \n",
      "    use_pseudocount : bool, optional\n",
      "            Whether to use the pseudocounts defined in the `add_edge` method\n",
      "            for edge-specific pseudocounts when updating the transition\n",
      "            probability parameters. Does not effect the `transition_pseudocount`\n",
      "            and `emission_pseudocount` parameters, but can be used in addition\n",
      "            to them. Default is False.\n",
      "    \n",
      "    stop_threshold : double, optional\n",
      "            The threshold the improvement ratio of the models log probability\n",
      "            in fitting the scores. Default is 1e-9.\n",
      "    \n",
      "    min_iterations : int, optional\n",
      "            The minimum number of iterations to run Baum-Welch training for.\n",
      "            Default is 0.\n",
      "    \n",
      "    max_iterations : int, optional\n",
      "            The maximum number of iterations to run Baum-Welch training for.\n",
      "            Default is 1e8.\n",
      "    \n",
      "    n_init : int, optional\n",
      "            The number of times to initialize the k-means clustering before\n",
      "            taking the best value. Default is 1.\n",
      "    \n",
      "    init : str, optional\n",
      "            The initialization method for kmeans. Must be one of 'first-k',\n",
      "            'random', 'kmeans++', or 'kmeans||'. Default is kmeans++.\n",
      "    \n",
      "    max_kmeans_iterations : int, optional\n",
      "            The number of iterations to run k-means for before starting EM.\n",
      "    \n",
      "    initialization_batch_size : int or None, optional\n",
      "            The number of batches to use to initialize the model. None means\n",
      "            use the entire data set. Default is None. \n",
      "    \n",
      "    batches_per_epoch : int or None, optional\n",
      "            The number of batches in an epoch. This is the number of batches to\n",
      "            summarize before calling `from_summaries` and updating the model\n",
      "            parameters. This allows one to do minibatch updates by updating the\n",
      "            model parameters before setting the full dataset. If set to None,\n",
      "            uses the full dataset. Default is None.\n",
      "    \n",
      "    lr_decay : double, optional, positive\n",
      "            The step size decay as a function of the number of iterations.\n",
      "            Functionally, this sets the inertia to be (2+k)^{-lr_decay}\n",
      "            where k is the number of iterations. This causes initial\n",
      "            iterations to have more of an impact than later iterations,\n",
      "            and is frequently used in minibatch learning. This value is\n",
      "            suggested to be between 0.5 and 1. Default is 0, meaning no\n",
      "            decay.\n",
      "    \n",
      "    end_state : bool, optional\n",
      "            Whether to calculate the probability of ending in each state or not.\n",
      "            Default is False.\n",
      "    \n",
      "    state_names : array-like, shape (n_states), optional\n",
      "            The name of the states. If None is passed in, default names are\n",
      "            generated. Default is None\n",
      "    \n",
      "    name : str, optional\n",
      "            The name of the model. Default is None\n",
      "    \n",
      "    keys : list\n",
      "            A list of sets where each set is the keys present in that column.\n",
      "            If there are d columns in the data set then this list should have\n",
      "            d sets and each set should have at least two keys in it.\n",
      "    \n",
      "    random_state : int, numpy.random.RandomState, or None\n",
      "            The random state used for generating samples. If set to none, a\n",
      "            random seed will be used. If set to either an integer or a\n",
      "            random seed, will produce deterministic outputs.\n",
      "    \n",
      "    callbacks : list, optional\n",
      "            A list of callback objects that describe functionality that should\n",
      "            be undertaken over the course of training.\n",
      "    \n",
      "    return_history : bool, optional\n",
      "            Whether to return the history during training as well as the model.\n",
      "    \n",
      "    verbose : bool, optional\n",
      "            Whether to print the improvement in the model fitting at each\n",
      "            iteration. Default is True.\n",
      "    \n",
      "    n_jobs : int, optional\n",
      "            The number of threads to use when performing training. This\n",
      "            leads to exact updates. Default is 1.\n",
      "    \n",
      "    multiple_check_input : bool, optional\n",
      "            Whether to check and transcode input at each iteration. This\n",
      "            leads to copying whole input data in each iteration. Which \n",
      "            can introduce significant overhead (up to 2 times slower) so \n",
      "            should be turned off when you know that data won't be changed \n",
      "            between fitting iteration. Default is True.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    model : HiddenMarkovModel\n",
      "            The model fit to the data.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(HiddenMarkovModel.from_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "tTsi-XMnKErh",
    "outputId": "fd7831e2-f86d-43bc-91ca-1ed9d24a02ee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zb/k8zh8whx36g93snxwz8xsqq00000gn/T/ipykernel_62451/2431243336.py:1: RuntimeWarning: invalid value encountered in divide\n",
      "  model = HiddenMarkovModel.from_samples(IndependentComponentsDistribution,\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "probabilities contain NaN",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[290], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mHiddenMarkovModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mIndependentComponentsDistribution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchr1_mut\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/hmm/lib/python3.10/site-packages/pomegranate/hmm.pyx:3711\u001b[0m, in \u001b[0;36mpomegranate.hmm.HiddenMarkovModel.from_samples\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/hmm/lib/python3.10/site-packages/pomegranate/kmeans.pyx:447\u001b[0m, in \u001b[0;36mpomegranate.kmeans.Kmeans.fit\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/hmm/lib/python3.10/site-packages/pomegranate/kmeans.pyx:458\u001b[0m, in \u001b[0;36mpomegranate.kmeans.Kmeans.fit\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/hmm/lib/python3.10/site-packages/pomegranate/kmeans.pyx:40\u001b[0m, in \u001b[0;36mpomegranate.kmeans._initialize_centroids\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/hmm/lib/python3.10/site-packages/pomegranate/kmeans.pyx:132\u001b[0m, in \u001b[0;36mpomegranate.kmeans._initialize_centroids\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mmtrand.pyx:954\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: probabilities contain NaN"
     ]
    }
   ],
   "source": [
    "model = HiddenMarkovModel.from_samples(JointProbabilityTable, n_components = 100, X=chr1, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
      "https://github.com/hmmlearn/hmmlearn/issues/335\n",
      "https://github.com/hmmlearn/hmmlearn/issues/340\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialHMM(n_components = 500,random_state = 0,n_trials=9,n_iter=400,tol=-1e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "chr1_mut = np.vstack([np.vstack([chr1[:,c],(chr1[:,c]==0).astype(int)]) for c in range(chr1.shape[1])],dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 1246254)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr1_mut.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  44559, 1201695,   48961, 1197293,   18491, 1227763,   37756,\n",
       "       1208498,   61010, 1185244,   26126, 1220128, 1111450,  134804,\n",
       "        111606, 1134648,    8557, 1237697])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr1_mut.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_mut = obs.iloc[:,3:].to_numpy()\n",
    "obs_mut = np.vstack([np.vstack([obs_mut[:,c],(obs_mut[:,c]==0).astype(int)]) for c in range(obs_mut.shape[1])],dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zb/k8zh8whx36g93snxwz8xsqq00000gn/T/ipykernel_62451/943843744.py:1: DeprecationWarning: less that 14405178 samples in lengths array 18; support for silently dropping samples is deprecated and will be removed\n",
      "  model.fit(obs_mut.T, len(obs_mut))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {color: black;background-color: white;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialHMM(n_components=500, n_iter=400, n_trials=9,\n",
       "               random_state=RandomState(MT19937) at 0x205535840, tol=-10000.0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialHMM</label><div class=\"sk-toggleable__content\"><pre>MultinomialHMM(n_components=500, n_iter=400, n_trials=9,\n",
       "               random_state=RandomState(MT19937) at 0x205535840, tol=-10000.0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialHMM(n_components=500, n_iter=400, n_trials=9,\n",
       "               random_state=RandomState(MT19937) at 0x205535840, tol=-10000.0)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(obs_mut.T, len(obs_mut))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprob, received = model.decode(chr1_mut.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-inf"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on MultinomialHMM in module hmmlearn.hmm object:\n",
      "\n",
      "class MultinomialHMM(hmmlearn.base.BaseHMM)\n",
      " |  MultinomialHMM(n_components=1, n_trials=None, startprob_prior=1.0, transmat_prior=1.0, algorithm='viterbi', random_state=None, n_iter=10, tol=0.01, verbose=False, params='ste', init_params='ste', implementation='log')\n",
      " |  \n",
      " |  Hidden Markov Model with multinomial emissions\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  n_features : int\n",
      " |      Number of possible symbols emitted by the model (in the samples).\n",
      " |  \n",
      " |  monitor_ : ConvergenceMonitor\n",
      " |      Monitor object used to check the convergence of EM.\n",
      " |  \n",
      " |  startprob_ : array, shape (n_components, )\n",
      " |      Initial state occupation distribution.\n",
      " |  \n",
      " |  transmat_ : array, shape (n_components, n_components)\n",
      " |      Matrix of transition probabilities between states.\n",
      " |  \n",
      " |  emissionprob_ : array, shape (n_components, n_features)\n",
      " |      Probability of emitting a given symbol when in each state.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from hmmlearn.hmm import MultinomialHMM\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MultinomialHMM\n",
      " |      hmmlearn.base.BaseHMM\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_components=1, n_trials=None, startprob_prior=1.0, transmat_prior=1.0, algorithm='viterbi', random_state=None, n_iter=10, tol=0.01, verbose=False, params='ste', init_params='ste', implementation='log')\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n_components : int\n",
      " |          Number of states\n",
      " |      \n",
      " |      n_trials : int\n",
      " |          Number of trials\n",
      " |          for now assume all samples have the same n_trials\n",
      " |      \n",
      " |      startprob_prior : array, shape (n_components, ), optional\n",
      " |          Parameters of the Dirichlet prior distribution for\n",
      " |          :attr:`startprob_`.\n",
      " |      \n",
      " |      transmat_prior : array, shape (n_components, n_components), optional\n",
      " |          Parameters of the Dirichlet prior distribution for each row\n",
      " |          of the transition probabilities :attr:`transmat_`.\n",
      " |      \n",
      " |      algorithm : {\"viterbi\", \"map\"}, optional\n",
      " |          Decoder algorithm.\n",
      " |      \n",
      " |      random_state: RandomState or an int seed, optional\n",
      " |          A random number generator instance.\n",
      " |      \n",
      " |      n_iter : int, optional\n",
      " |          Maximum number of iterations to perform.\n",
      " |      \n",
      " |      tol : float, optional\n",
      " |          Convergence threshold. EM will stop if the gain in log-likelihood\n",
      " |          is below this value.\n",
      " |      \n",
      " |      verbose : bool, optional\n",
      " |          Whether per-iteration convergence reports are printed to\n",
      " |          :data:`sys.stderr`.  Convergence can also be diagnosed using the\n",
      " |          :attr:`monitor_` attribute.\n",
      " |      \n",
      " |      params, init_params : string, optional\n",
      " |          The parameters that get updated during (``params``) or initialized\n",
      " |          before (``init_params``) the training.  Can contain any\n",
      " |          combination of 's' for startprob, 't' for transmat, and 'e' for\n",
      " |          emissionprob.  Defaults to all parameters.\n",
      " |      \n",
      " |      implementation: string, optional\n",
      " |          Determines if the forward-backward algorithm is implemented with\n",
      " |          logarithms (\"log\"), or using scaling (\"scaling\").  The default is\n",
      " |          to use logarithms for backwards compatability.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from hmmlearn.base.BaseHMM:\n",
      " |  \n",
      " |  decode(self, X, lengths=None, algorithm=None)\n",
      " |      Find most likely state sequence corresponding to ``X``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          Feature matrix of individual samples.\n",
      " |      lengths : array-like of integers, shape (n_sequences, ), optional\n",
      " |          Lengths of the individual sequences in ``X``. The sum of\n",
      " |          these should be ``n_samples``.\n",
      " |      algorithm : string\n",
      " |          Decoder algorithm. Must be one of \"viterbi\" or \"map\".\n",
      " |          If not given, :attr:`decoder` is used.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      log_prob : float\n",
      " |          Log probability of the produced state sequence.\n",
      " |      state_sequence : array, shape (n_samples, )\n",
      " |          Labels for each sample from ``X`` obtained via a given\n",
      " |          decoder ``algorithm``.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      score_samples : Compute the log probability under the model and\n",
      " |          posteriors.\n",
      " |      score : Compute the log probability under the model.\n",
      " |  \n",
      " |  fit(self, X, lengths=None)\n",
      " |      Estimate model parameters.\n",
      " |      \n",
      " |      An initialization step is performed before entering the\n",
      " |      EM algorithm. If you want to avoid this step for a subset of\n",
      " |      the parameters, pass proper ``init_params`` keyword argument\n",
      " |      to estimator's constructor.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          Feature matrix of individual samples.\n",
      " |      lengths : array-like of integers, shape (n_sequences, )\n",
      " |          Lengths of the individual sequences in ``X``. The sum of\n",
      " |          these should be ``n_samples``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns self.\n",
      " |  \n",
      " |  get_stationary_distribution(self)\n",
      " |      Compute the stationary distribution of states.\n",
      " |  \n",
      " |  predict(self, X, lengths=None)\n",
      " |      Find most likely state sequence corresponding to ``X``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          Feature matrix of individual samples.\n",
      " |      lengths : array-like of integers, shape (n_sequences, ), optional\n",
      " |          Lengths of the individual sequences in ``X``. The sum of\n",
      " |          these should be ``n_samples``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      state_sequence : array, shape (n_samples, )\n",
      " |          Labels for each sample from ``X``.\n",
      " |  \n",
      " |  predict_proba(self, X, lengths=None)\n",
      " |      Compute the posterior probability for each state in the model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          Feature matrix of individual samples.\n",
      " |      lengths : array-like of integers, shape (n_sequences, ), optional\n",
      " |          Lengths of the individual sequences in ``X``. The sum of\n",
      " |          these should be ``n_samples``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      posteriors : array, shape (n_samples, n_components)\n",
      " |          State-membership probabilities for each sample from ``X``.\n",
      " |  \n",
      " |  sample(self, n_samples=1, random_state=None, currstate=None)\n",
      " |      Generate random samples from the model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n_samples : int\n",
      " |          Number of samples to generate.\n",
      " |      random_state : RandomState or an int seed\n",
      " |          A random number generator instance. If ``None``, the object's\n",
      " |          ``random_state`` is used.\n",
      " |      currstate : int\n",
      " |          Current state, as the initial state of the samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : array, shape (n_samples, n_features)\n",
      " |          Feature matrix.\n",
      " |      state_sequence : array, shape (n_samples, )\n",
      " |          State sequence produced by the model.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      ::\n",
      " |      \n",
      " |          # generate samples continuously\n",
      " |          _, Z = model.sample(n_samples=10)\n",
      " |          X, Z = model.sample(n_samples=10, currstate=Z[-1])\n",
      " |  \n",
      " |  score(self, X, lengths=None)\n",
      " |      Compute the log probability under the model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          Feature matrix of individual samples.\n",
      " |      lengths : array-like of integers, shape (n_sequences, ), optional\n",
      " |          Lengths of the individual sequences in ``X``. The sum of\n",
      " |          these should be ``n_samples``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      log_prob : float\n",
      " |          Log likelihood of ``X``.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      score_samples : Compute the log probability under the model and\n",
      " |          posteriors.\n",
      " |      decode : Find most likely state sequence corresponding to ``X``.\n",
      " |  \n",
      " |  score_samples(self, X, lengths=None)\n",
      " |      Compute the log probability under the model and compute posteriors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          Feature matrix of individual samples.\n",
      " |      lengths : array-like of integers, shape (n_sequences, ), optional\n",
      " |          Lengths of the individual sequences in ``X``. The sum of\n",
      " |          these should be ``n_samples``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      log_prob : float\n",
      " |          Log likelihood of ``X``.\n",
      " |      posteriors : array, shape (n_samples, n_components)\n",
      " |          State-membership probabilities for each sample in ``X``.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      score : Compute the log probability under the model.\n",
      " |      decode : Find most likely state sequence corresponding to ``X``.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        , 55.55555556,  0.        , 55.55555556,  0.        ,\n",
       "       55.55555556,  0.        , 55.55555556,  0.        , 55.55555556,\n",
       "        0.        , 55.55555556,  0.        , 55.55555556,  0.        ,\n",
       "       55.55555556,  0.        , 55.55555556])"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.emissionprob_.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvergenceMonitor(\n",
       "    history=[-257.26974385281085, -125.51748688700107, -125.51748688700104, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700104, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700104, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700104, -125.51748688700106, -125.51748688700106, -125.51748688700107, -125.51748688700107, -125.51748688700107, -125.51748688700104, -125.51748688700106, -125.51748688700106, -125.51748688700107, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700104, -125.51748688700104, -125.51748688700107, -125.51748688700106, -125.51748688700106, -125.51748688700107, -125.51748688700106, -125.51748688700106, -125.51748688700107, -125.51748688700106, -125.51748688700107, -125.51748688700106, -125.51748688700107, -125.51748688700107, -125.51748688700104, -125.51748688700104, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700104, -125.51748688700106, -125.51748688700106, -125.51748688700104, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700107, -125.51748688700104, -125.51748688700106, -125.51748688700107, -125.51748688700106, -125.51748688700107, -125.51748688700106, -125.51748688700104, -125.51748688700104, -125.51748688700106, -125.51748688700106, -125.51748688700104, -125.51748688700104, -125.51748688700107, -125.51748688700106, -125.51748688700104, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700104, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700104, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700104, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700107, -125.51748688700106, -125.51748688700104, -125.51748688700104, -125.51748688700106, -125.51748688700107, -125.51748688700107, -125.51748688700106, -125.51748688700106, -125.51748688700104, -125.51748688700106, -125.51748688700106, -125.51748688700104, -125.51748688700104, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700107, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700107, -125.51748688700104, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700104, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700107, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700107, -125.51748688700104, -125.51748688700106, -125.51748688700104, -125.51748688700106, -125.51748688700106, -125.51748688700104, -125.51748688700107, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700104, -125.51748688700106, -125.51748688700104, -125.51748688700107, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700107, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700107, -125.51748688700104, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700104, -125.51748688700106, -125.51748688700106, -125.51748688700104, -125.51748688700104, -125.51748688700107, -125.51748688700106, -125.51748688700107, -125.51748688700104, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700107, -125.51748688700106, -125.51748688700107, -125.51748688700107, -125.51748688700106, -125.51748688700106, -125.51748688700107, -125.51748688700107, -125.51748688700106, -125.51748688700106, -125.51748688700107, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700107, -125.51748688700107, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700104, -125.51748688700107, -125.51748688700106, -125.51748688700106, -125.51748688700107, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700104, -125.51748688700106, -125.51748688700107, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700107, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700107, -125.51748688700107, -125.51748688700106, -125.51748688700107, -125.51748688700107, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700107, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700107, -125.51748688700107, -125.51748688700106, -125.51748688700106, -125.51748688700107, -125.51748688700107, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700104, -125.51748688700106, -125.51748688700106, -125.51748688700107, -125.51748688700104, -125.51748688700106, -125.51748688700107, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700107, -125.51748688700104, -125.51748688700104, -125.51748688700106, -125.51748688700104, -125.51748688700104, -125.51748688700107, -125.51748688700106, -125.51748688700107, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700104, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700104, -125.51748688700106, -125.51748688700104, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700107, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700108, -125.51748688700106, -125.51748688700107, -125.51748688700106, -125.51748688700107, -125.51748688700104, -125.51748688700106, -125.51748688700107, -125.51748688700107, -125.51748688700107, -125.51748688700107, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700107, -125.51748688700107, -125.51748688700107, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700107, -125.51748688700107, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700107, -125.51748688700107, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700107, -125.51748688700104, -125.51748688700106, -125.51748688700107, -125.51748688700106, -125.51748688700107, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700104, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700104, -125.51748688700104, -125.51748688700106, -125.51748688700104, -125.51748688700106, -125.51748688700104, -125.51748688700106, -125.51748688700106, -125.51748688700104, -125.51748688700106, -125.51748688700106, -125.51748688700104, -125.51748688700104, -125.51748688700104, -125.51748688700106, -125.51748688700104, -125.51748688700104, -125.51748688700107, -125.51748688700106, -125.51748688700106, -125.51748688700106, -125.51748688700107, -125.51748688700106, -125.51748688700106, -125.51748688700104, -125.51748688700106, -125.51748688700106, -125.51748688700104, -125.51748688700106, -125.51748688700104],\n",
       "    iter=400,\n",
       "    n_iter=400,\n",
       "    tol=-10000.0,\n",
       "    verbose=False,\n",
       ")"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.monitor_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['chr', 'start', 'end', 'H3K4me2', 'H3K9Ac', 'H3K4me3', 'ATAC-seq',\n",
       "       'H3K4me1', 'H3K27Ac', 'HiC', 'H3K27me3', 'RNA-Pol_ChIPseq'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AgHV6xFYIht5"
   },
   "outputs": [],
   "source": [
    "dist = MultivariateGaussianDistribution.from_samples(obs.iloc[:, 3:].to_numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c8K1TtJUtMnk",
    "outputId": "6a976b12-7bf9-491f-fe4d-2d4a3255a188"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"class\" : \"Distribution\",\n",
       "    \"name\" : \"MultivariateGaussianDistribution\",\n",
       "    \"parameters\" : [\n",
       "        [\n",
       "            0.12134942032649648,\n",
       "            0.12134935090701413,\n",
       "            0.12137024617120316,\n",
       "            0.12140370636169855,\n",
       "            0.12137788231426228,\n",
       "            0.12140453939548682,\n",
       "            0.12139523718485117,\n",
       "            0.12139607021863943,\n",
       "            0.12137024617120316\n",
       "        ],\n",
       "        [\n",
       "            [\n",
       "                0.10662373851291976,\n",
       "                0.10128705481136609,\n",
       "                0.09666722115325244,\n",
       "                0.09392595058925125,\n",
       "                0.09231174922251781,\n",
       "                0.09116767462813201,\n",
       "                0.090252327439942,\n",
       "                0.08947084065838028,\n",
       "                0.08879137262156474\n",
       "            ],\n",
       "            [\n",
       "                0.10128705481136609,\n",
       "                0.10662368594146047,\n",
       "                0.10132902549681877,\n",
       "                0.09670398786193933,\n",
       "                0.0939068785140741,\n",
       "                0.09232441989046333,\n",
       "                0.0911994952843961,\n",
       "                0.09026910371323946,\n",
       "                0.08947106719876789\n",
       "            ],\n",
       "            [\n",
       "                0.09666722115325244,\n",
       "                0.10132902549681877,\n",
       "                0.1066395095155447,\n",
       "                0.10134915486260987,\n",
       "                0.09666800130321447,\n",
       "                0.09391526848904656,\n",
       "                0.09234293551356976,\n",
       "                0.09119574688161017,\n",
       "                0.09026977080075756\n",
       "            ],\n",
       "            [\n",
       "                0.09392595058925125,\n",
       "                0.09670398786193933,\n",
       "                0.10134915486260987,\n",
       "                0.10666484644334104,\n",
       "                0.10132087653049208,\n",
       "                0.09669180363578415,\n",
       "                0.09392330387109209,\n",
       "                0.09233911956983068,\n",
       "                0.09119315401347076\n",
       "            ],\n",
       "            [\n",
       "                0.09231174922251781,\n",
       "                0.0939068785140741,\n",
       "                0.09666800130321447,\n",
       "                0.10132087653049208,\n",
       "                0.10664529199916738,\n",
       "                0.1013391715814392,\n",
       "                0.09668995896054901,\n",
       "                0.09392314437939021,\n",
       "                0.09234046019264992\n",
       "            ],\n",
       "            [\n",
       "                0.09116767462813201,\n",
       "                0.09232441989046333,\n",
       "                0.09391526848904656,\n",
       "                0.09669180363578415,\n",
       "                0.1013391715814392,\n",
       "                0.10666547720965651,\n",
       "                0.10135691859332278,\n",
       "                0.09668460861877941,\n",
       "                0.09393776040132977\n",
       "            ],\n",
       "            [\n",
       "                0.090252327439942,\n",
       "                0.0911994952843961,\n",
       "                0.09234293551356976,\n",
       "                0.09392330387109209,\n",
       "                0.09668995896054901,\n",
       "                0.10135691859332278,\n",
       "                0.1066584335736849,\n",
       "                0.10131386533975914,\n",
       "                0.09667852928408754\n",
       "            ],\n",
       "            [\n",
       "                0.08947084065838028,\n",
       "                0.09026910371323946,\n",
       "                0.09119574688161017,\n",
       "                0.09233911956983068,\n",
       "                0.09392314437939021,\n",
       "                0.09668460861877941,\n",
       "                0.10131386533975914,\n",
       "                0.1066590643541106,\n",
       "                0.101364382076538\n",
       "            ],\n",
       "            [\n",
       "                0.08879137262156474,\n",
       "                0.08947106719876789,\n",
       "                0.09026977080075756,\n",
       "                0.09119315401347076,\n",
       "                0.09234046019264992,\n",
       "                0.09393776040132977,\n",
       "                0.09667852928408754,\n",
       "                0.101364382076538,\n",
       "                0.1066395095155447\n",
       "            ]\n",
       "        ]\n",
       "    ],\n",
       "    \"frozen\" : false\n",
       "}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "thhGRGVhPUYc",
    "outputId": "a74f72cd-cf19-4f34-c5a3-4e5d6f686403"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nonLAD', 'LAD']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://github.com/keoughkath/LAD_atlas/blob/main/HMM_QC.ipynb as a reference to use bedgraph files in pomegranate \n",
    "\n",
    "states_dict = {\n",
    "        2:['nonLAD','LAD'],\n",
    "        3:['nonLAD','T2-LAD','T1-LAD'],\n",
    "        4:['nonLAD','T3-LAD','T2-LAD','T1-LAD'],\n",
    "        5:['nonLAD','T4-LAD','T3-LAD','T2-LAD','T1-LAD']\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
